### 书生·浦语大模型全链路开源体系”的学习笔记
书生·浦语大模型在去年6月份开源，不断发展、迭代升级。今年1月17号，InternLM 2开源，包括7B轻量级模型和20B中量级模型。InternLM通过新一代数据清洗过滤技术构建了高质量语料库。InternLM 2比InternLM 1的语言建模能力强很多。书生·浦语在数据、预训练、微调、部署、评测和应用完成了全链条开源开放体系。其中，高效微调框架Xtuner最低只需8G显存即可微调7B模型。
